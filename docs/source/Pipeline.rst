=============================================
Metagenomics Pipeline, From raw reads to bins
=============================================

Basic Pipeline Structure
========================
Currently, this pipeline covers:

- Quality control
    - adapter trimming
    - quality trimming
- Assembly with MEGAHIT
    - single sample assemblies
    - co-assemblies
- Mapping with bwa
    - can map multiple samples to one reference
    - generates sorted bams
- Binning with concoct
    - runs the multithreaded version of concoct
    - checkm
- Anvio
 
This pipeline is simply a script that leverages the slurm job scheduler. It creates a series of jobs that depend on the successful run of prereq jobs and submits them all to slurm. Slurm then handles the run order and logging of errors. This pipelines does not need to be kept running and creates a separate job for each step with a detailed log file name to identify log for each step run. 

This pipeline allows for large projects with multiple assemblies and data sets to be submitted for efficient analysis in a simplistic way. It is modular and you can use and workflow at any time as long as you have the correct input files. This means if you already have QCed data you can start at assembly without running QC from this pipeline specifically and so on.

Before You Can Run
==================
Conda Env
-------------
I have created a conda env in our shared storage directory. This env contains all requirements and it can be loaded using the command:



.. code-block:: bash

    conda activate /nfs/turbo/lsa-dudelabs/conda_envs/miniconda/envs/metaG_pipeline/

Singularity
------------
All software in the pipeline is installed in singularity containers in the shared directory. In order to use them the singularity module on greatlakes must be loaded.

.. code-block:: bash

    module load singularity


The Files
=========

The CSV Files
-------------
There are three csv files that are used by the pipeline to supply what files you want to run through it. These include the fastq, assembly, and mapping csv files. It is important to note that the names you use for samples and assemblies are what they will be known as to the pipeline and will be what the outfiles use in their naming scheme. It is also important to note that the column names in your file must match the names in these examples. LETTER CASE MATTERS.
The sample name column here cannot be only numbers with an _. For example, 1110009_994 will not work for some reason.
- The fastq csv:
    The fastq csv maps sample names to the raw reads paths as well as the qced fastq files for mapping and assembly later. the qced reads that are generated by this pipelines qc will be in the form trimmed_reads/{sample}/{sample}_trimmed_R1.fastq where {sample} is the first column of this csv. The full table can be populated before the run. To run qc you only need sample_name, raw_fq1, and raw_fq2. To run assembly and Mapping, you need sample_name, qced_fq1, and qced_fq2. 

    .. code-block:: csv
        sample_name,raw_fq1,raw_fq2,qced_fq1,qced_fq2
        sample_77,raw_reads/100920_77/36_D3_S77_R1_001.fastq.gz,raw_reads/100920_77/36_D3_S77_R2_001.fastq.gz,trimmed_reads/sample_77/sample_77_trimmed_R1.fastq,trimmed_reads/sample_77/
        sample_77_trimmed_R2.fastq
        sample_78,raw_reads/100920_78/30_D3_S78_R1_001.fastq.gz,raw_reads/100920_78/30_D3_S78_R2_001.fastq.gz,trimmed_reads/sample_78/sample_78_trimmed_R1.fastq,trimmed_reads/sample_78/
        sample_78_trimmed_R2.fastq
        sample_79,raw_reads/100920_79/31_D3_S79_R1_001.fastq.gz,raw_reads/100920_79/31_D3_S79_R2_001.fastq.gz,trimmed_reads/sample_79/sample_79_trimmed_R1.fastq,trimmed_reads/sample_79/
        sample_79_trimmed_R2.fastq
        sample_80,raw_reads/100920_80/7_D31_S80_R1_001.fastq.gz,raw_reads/100920_80/7_D31_S80_R2_001.fastq.gz,trimmed_reads/sample_80/sample_80_trimmed_R1.fastq,trimmed_reads/sample_80/
        sample_80_trimmed_R2.fastq
        sample_81,raw_reads/100920_81/28_D31_S81_R1_001.fastq.gz,raw_reads/100920_81/28_D31_S81_R2_001.fastq.gz,trimmed_reads/sample_81/sample_81_trimmed_R1.fastq,trimmed_reads/sample_8
        1/sample_81_trimmed_R2.fastq 

- The Assembly csv:
 The Assembly csv keeps track of the assembly paths once assembly has completed. This file is used for binning and mapping when the assembly is 
 is being used as the reference. The name refers to whatever you named your assembly, and the path to the final contigs from megahit.

 .. code-block:: csv
    name,path
    sample_77,assemblies/sample_77/Megahit_meta-sensitive_out/final.contigs.fa
    sample_78,assemblies/sample_78/Megahit_meta-sensitive_out/final.contigs.fa
    sample_79,assemblies/sample_79/Megahit_meta-sensitive_out/final.contigs.fa
    sample_80,assemblies/sample_80/Megahit_meta-sensitive_out/final.contigs.fa
    sample_81,assemblies/sample_81/Megahit_meta-sensitive_out/final.contigs.fa

- The Mapping csv:
 The Mapping csv keeps track of the path to the bam files and the reference and sample that were used to create them. The first column being the reference you mapped
 to and the second being the name of the sample being mapped. This file is used by the binning workflow to create coverage files.
 
 .. code-block:: csv
    ref,sample,bam
    sample_77,sample_77,mapping/sample_77/sample_77_mapped_sorted.bam
    sample_77,sample_78,mapping/sample_77/sample_78_mapped_sorted.bam
    sample_77,sample_79,mapping/sample_77/sample_79_mapped_sorted.bam
    sample_77,sample_80,mapping/sample_77/sample_80_mapped_sorted.bam
    sample_77,sample_81,mapping/sample_77/sample_81_mapped_sorted.bam
    sample_78,sample_77,mapping/sample_78/sample_77_mapped_sorted.bam
    sample_78,sample_78,mapping/sample_78/sample_78_mapped_sorted.bam
    sample_78,sample_79,mapping/sample_78/sample_79_mapped_sorted.bam
    sample_78,sample_80,mapping/sample_78/sample_80_mapped_sorted.bam
    sample_78,sample_81,mapping/sample_78/sample_81_mapped_sorted.bam

The Scheme Files
----------------
The assembly, mapping, binning, and anvio workflows require these yaml files. They are used to include multiple samples for co-assembly, mapping multiple samples to the same reference, and indicating what mappings to include when computing coverage for binning or visualization in anvio.
All of these follow the same scheme. The first entry for assembly is the name of the assembly and the indented entries under it are all the samples you want to be
part of that assembly. The entries must have the same name as the samples in the fastq csv as that is where the path to the fastq will be pulled from.
For the mapping file the first entry is the reference you are mapping to that must match to an assembly in the assembly.csv file and the entries under it
are all of the samples you want to map to it that must again match to the fastq.csv file. For binning the first entry is the assembly you want to bin and the entries under it are the mappings you want to include (this may be exactly the same as the mapping scheme assuming you map all vs all for binning). In most cases your mapping, binning, and anvio yaml files will be exactly the same so you can make one and use it for each workflow.



.. code-block:: yaml 

    sample_77:
    - sample_77
    - sample_78
    - sample_79
    - sample_80
    - sample_81
    sample_78:
    - sample_77
    - sample_78
    - sample_79
    - sample_80
    - sample_81


Running The Pipeline
====================

The pipeline consists of separate workflows that are each called indiviually. You will have to wait for the previously run workflow to finish before running the next. For example you cannot run assembly until ALL of the quality control jobs are done running.

The current workflows and their steps are as follows:

- assemble
    - run megahit meta-sensitive
    - run binstats.sh from bbtools
.. code-block:: bash

    mgjss assemble fastq_info.csv assembly_scheme.yml assembly_output --account vdenef1
       
- map 
    - run bwa mem default
    - run samtools sort
    - run samtools index
.. code-block:: bash

    mgjss map fastq_info.csv assembly_info.csv mapping_scheme.yml mapping_output --account vdenef1
    
- concoct
    - cut contigs into 10k chunks
    - generate coverage table
    - run concoct
    - merge clustering
    - extract bins into fasta files
    - run checkm on extracted bins
.. code-block:: bash

    mgjss concoct assembly_info.csv mapping_info.csv binning_scheme.yaml binning_output --account vdenef1
    
- anvio
    - create contigs db
    - run hmm profiles
    - create profile dbs
    - merge profile dbs (IF MORE THAN ONE PER CONTIGS DB)
    - import bins to contig db
    - IF RENAME FLAG USED
        - rename contigs with anvio
        - use the new contig names to update bam contig names
        - use the new contig names to update binlist contig names
        - run the above steps
.. code-block:: bash

    mgjss assemble assembly_info.csv bam_info.csv binning_info.csv anvio_scheme.yaml anvio_output --rename_contigs --account vdenef1
